{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import gensim\n",
    "#import gensim.downloader as api\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GlobalMaxPool1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "#warnings.filterwarnings('ignore')\n",
    "#v2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_TYPE = 'word2vec'\n",
    "MODEL_TYPE = 'bilstm'\n",
    "DIM_HIDDEN_1 = 150\n",
    "DIM_HIDDEN_2 = 150\n",
    "DROPOUT_RATE = 0.1\n",
    "SEQ_LEN = 220\n",
    "EMBED_DIM = 300\n",
    "VOCAB_SIZE = 30_000\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "STAMP = f'{EMBED_TYPE}-{MODEL_TYPE}-{DIM_HIDDEN_1}-{DIM_HIDDEN_2}-{DROPOUT_RATE}_le_{SEQ_LEN}_em_{EMBED_DIM}_lr_{LR}_ep_{NUM_EPOCHS}_ba_{BATCH_SIZE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10400, 81)\n",
      "(2600, 81)\n"
     ]
    }
   ],
   "source": [
    "TEXT_COLS = ['name', 'description', 'space', 'house_rules', 'access', 'interaction', 'neighborhood_overview', 'notes', 'transit']\n",
    "INPUT_COLS = ['description']\n",
    "\n",
    "df_train = pd.read_csv('./input/df_pre_train_deep.csv')\n",
    "df_test = pd.read_csv('./input/df_pre_test_deep.csv')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(string):\n",
    "    '''Clean the text and return list of tokens'''  \n",
    "    if pd.isna(string):\n",
    "        return ''\n",
    "    string = str(string)#.lower()\n",
    "    string = re.sub(\"&\", \" and \", string)\n",
    "    string = re.sub(\"\\.\\.\\.\", \".\", string)\n",
    "    string = re.sub(\"\\.\\.\", \".\", string)\n",
    "    string = re.sub(\"-\", \" \", string)\n",
    "    string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(\"/\", \" \", string)\n",
    "    string = re.sub(\"\\â€™\", \"\\'\", string)\n",
    "    string = re.sub(\"what's\", \"what is\", string)\n",
    "    string = re.sub(\"What's\", \"What is\", string)\n",
    "    string = re.sub(\"it\\'s\", \"it is\", string)\n",
    "    string = re.sub(\"It\\'s\", \"It is\", string)\n",
    "    string = re.sub(\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(\"can\\'t\", \"can not\", string)\n",
    "    string = re.sub(\"Can\\'t\", \"Can not\", string)\n",
    "    string = re.sub(\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(\"I\\'m\", \"I am\", string)\n",
    "    string = re.sub(\"\\'re\", \" are\", string)\n",
    "    string = re.sub(\"\\'d\", \" would\", string)\n",
    "    string = re.sub(\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(\"he\\'s\", \"he is\", string)\n",
    "    string = re.sub(\"He\\'s\", \"He is\", string)\n",
    "    string = re.sub(\"she\\'s\", \"she is\", string)\n",
    "    string = re.sub(\"She\\'s\", \"She is\", string)\n",
    "    string = re.sub(\"that\\'s\", \"that is\", string)\n",
    "    string = re.sub(\"That\\'s\", \"That is\", string)\n",
    "    string = re.sub(\"what\\'s\", \"what is\", string)\n",
    "    string = re.sub(\"What\\'s\", \"What is\", string)\n",
    "    string = re.sub(\"where\\'s\", \"where is\", string)\n",
    "    string = re.sub(\"Where\\'s\", \"Where is\", string)\n",
    "    string = re.sub(\"how\\'s\", \"how is\", string)\n",
    "    string = re.sub(\"How\\'s\", \"How is\", string)\n",
    "    string = re.sub(\"who\\'s\", \"who is\", string)\n",
    "    string = re.sub(\"Who\\'s\", \"Who is\", string)\n",
    "    string = re.sub(\"won\\'t\", \"will not\", string)\n",
    "    string = re.sub(\"Won\\'t\", \"Will not\", string)\n",
    "    string = re.sub(\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(\"n\\'\", \"ng\", string)\n",
    "    string = re.sub(\"\\'bout\", \"about\", string)\n",
    "    string = re.sub(\"\\'til\", \"until\", string)\n",
    "    string = re.sub(\"[^A-Za-z\\s]\", \"\", string)\n",
    "    text = string.split(\" \")    \n",
    "    return text\n",
    "\n",
    "def process_df(df):\n",
    "    '''Tokenize a whole dataframe'''\n",
    "    res = []\n",
    "    for i, row in df.iterrows():\n",
    "        seq = tokenize_string(row[INPUT_COLS].values)\n",
    "        seq = [token for token in seq if token != '']\n",
    "        res.append(seq) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = process_df(df_train)\n",
    "x_test = process_df(df_test)\n",
    "\n",
    "y_train = np.array(df_train['log_price'])\n",
    "y_test = np.array(df_test['log_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fully', 'furnished', 'one', 'bedroom', 'apartment', 'situated', 'on', 'the', 'rd', 'floor', 'in', 'a', 'well', 'maintained', 'building', 'on', 'Na', 'Kozace', 'street', 'in', 'Prague', 'Vinohrady', 'The', 'flat', 'has', 'an', 'entrance', 'hall', 'living', 'area', 'with', 'fully', 'equipped', 'kitchen', 'corner', 'bedroom', 'with', 'the', 'window', 'facing', 'into', 'quiet', 'court', 'and', 'shower', 'bathroom', 'washing', 'machine', 'and', 'dryer', 'Building', 'is', 'equipped', 'with', 'an', 'elevator', 'Close', 'to', 'tram', 'bus', 'and', 'metro', 'A', 'Nmst', 'Mru', 'Fully', 'furnished', 'one', 'bedroom', 'apartment', 'situated', 'on', 'the', 'rd', 'floor', 'in', 'a', 'well', 'maintained', 'building', 'on', 'Na', 'Kozace', 'street', 'in', 'Prague', 'Vinohrady', 'The', 'flat', 'has', 'an', 'entrance', 'hall', 'living', 'area', 'with', 'fully', 'equipped', 'kitchen', 'corner', 'bedroom', 'with', 'the', 'window', 'facing', 'into', 'quiet', 'court', 'and', 'shower', 'bathroom', 'washing', 'machine', 'and', 'dryer', 'Building', 'is', 'equipped', 'with', 'an', 'elevator', 'Close', 'to', 'tram', 'bus', 'and', 'metro', 'A', 'Nmst', 'Mru']\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35464 unique tokens in data\n"
     ]
    }
   ],
   "source": [
    "# --------- Create vocabulary index based on word frequency ---------\n",
    "tokenizer = Tokenizer(num_words = VOCAB_SIZE, filters = \"\", lower = False)\n",
    "tokenizer.fit_on_texts(x_train) \n",
    "\n",
    "# --------- Transform each text to a vector of integers ---------\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Found {len(word_index)} unique tokens in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1326, 75, 150, 4603, 130, 62, 2, 338, 10, 11, 21, 3, 175, 6, 75, 603, 924, 8, 45, 908, 29, 13, 452, 12, 3, 339, 8, 23, 422, 507, 195, 24, 67, 107, 14, 595, 837, 2, 114, 567, 75, 150, 415, 124, 17334, 4603, 130, 32, 4, 30, 123, 320, 8, 1039, 22, 20, 1, 127, 44, 6, 3, 94, 58, 55, 362, 6702, 5021, 1, 28, 2, 1, 17, 14, 1, 537, 6, 1, 11, 10, 28, 4, 39, 8, 3, 88, 3, 842, 3, 241, 2, 2467, 5, 7291, 584, 10, 17, 4, 39, 8, 3, 636, 656, 1049, 3, 201, 3, 295, 3, 376, 3, 639, 279, 3102, 324, 1083, 191, 18, 340, 40, 324, 12543, 511, 340, 3, 1155, 136, 534, 10, 371, 6, 1, 58, 4, 30, 123, 30, 532, 55, 117, 192, 1492, 5, 1, 44, 72, 1311, 55, 743, 4, 87, 56, 17335, 56, 5841, 6, 1, 551, 2, 1, 165, 242, 7, 1, 116, 406]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [PAD]\n",
      "1: the\n",
      "2: and\n",
      "3: a\n",
      "4: is\n",
      "5: to\n",
      "6: of\n",
      "7: in\n",
      "8: with\n",
      "9: you\n",
      "10: The\n",
      "35454: welches\n",
      "35455: letzten\n",
      "35456: Jahren\n",
      "35457: Szeneviertel\n",
      "35458: Kreative\n",
      "35459: Welt\n",
      "35460: entwickelte\n",
      "35461: gelangt\n",
      "35462: Fuss\n",
      "35463: bringen\n"
     ]
    }
   ],
   "source": [
    "print('0: [PAD]')\n",
    "for word, i in word_index.items(): \n",
    "    if (i <= 10) or (len(word_index)-11 < i < len(word_index)):\n",
    "        print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen = SEQ_LEN)\n",
    "x_test = pad_sequences(x_test, maxlen = SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0  1326    75   150  4603   130\n",
      "    62     2   338    10    11    21     3   175     6    75   603   924\n",
      "     8    45   908    29    13   452    12     3   339     8    23   422\n",
      "   507   195    24    67   107    14   595   837     2   114   567    75\n",
      "   150   415   124 17334  4603   130    32     4    30   123   320     8\n",
      "  1039    22    20     1   127    44     6     3    94    58    55   362\n",
      "  6702  5021     1    28     2     1    17    14     1   537     6     1\n",
      "    11    10    28     4    39     8     3    88     3   842     3   241\n",
      "     2  2467     5  7291   584    10    17     4    39     8     3   636\n",
      "   656  1049     3   201     3   295     3   376     3   639   279  3102\n",
      "   324  1083   191    18   340    40   324 12543   511   340     3  1155\n",
      "   136   534    10   371     6     1    58     4    30   123    30   532\n",
      "    55   117   192  1492     5     1    44    72  1311    55   743     4\n",
      "    87    56 17335    56  5841     6     1   551     2     1   165   242\n",
      "     7     1   116   406]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 valid word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "if EMBED_TYPE == 'glove':\n",
    "    with open('./input/glove.840B.300d.txt', encoding = 'utf8') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                word = parts[0]\n",
    "                coefs = np.asarray(parts[1:], dtype = 'float32')        \n",
    "            except:\n",
    "                pass\n",
    "            if coefs.shape == (EMBED_DIM,):\n",
    "                embeddings_dict[word] = coefs   \n",
    "elif EMBED_TYPE == 'word2vec':\n",
    "    v2v = gensim.models.KeyedVectors.load_word2vec_format('./input/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "    for token in v2v.vocab.keys():\n",
    "        coefs = np.asarray(v2v[token], dtype = 'float32')\n",
    "        if coefs.shape == (EMBED_DIM,):\n",
    "            embeddings_dict[token] = coefs   \n",
    "\n",
    "print(f\"Found {len(embeddings_dict.values())} valid word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (30001, 300)\n"
     ]
    }
   ],
   "source": [
    "# Update vocab size from maximum to actual tokens found (+ pad token)\n",
    "VOCAB_SIZE = min(VOCAB_SIZE, len(word_index)) + 1\n",
    "\n",
    "embed_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM))\n",
    "\n",
    "print(f\"Embedding matrix shape: {embed_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embedding for 21066 words\n"
     ]
    }
   ],
   "source": [
    "# --------- Search for embeddings from GloVe ---------\n",
    "num_found = 0\n",
    "for word, i in word_index.items(): # iterate over words found in data\n",
    "    if i >= VOCAB_SIZE: \n",
    "        continue # if words found in data > vocab size: skip iteration\n",
    "    embed_vector = embeddings_dict.get(word) # search for embedding from dict\n",
    "    if embed_vector is not None:\n",
    "        embed_matrix[i] = embed_vector # save vector to embedding matrix\n",
    "        num_found += 1\n",
    "        \n",
    "print(f\"Found embedding for {num_found} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 220)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 220, 300)          9000300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300)               541200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 9,586,801\n",
      "Trainable params: 586,501\n",
      "Non-trainable params: 9,000,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape = (SEQ_LEN,), dtype = 'int32')\n",
    "h = Embedding(VOCAB_SIZE, EMBED_DIM, weights = [embed_matrix], input_length = SEQ_LEN, trainable = False)(x)\n",
    "h = Bidirectional(LSTM(DIM_HIDDEN_1, return_sequences = False, dropout = DROPOUT_RATE, recurrent_dropout = DROPOUT_RATE), merge_mode = 'concat')(h)\n",
    "#h = GlobalMaxPool1D()(h)\n",
    "h = Dropout(DROPOUT_RATE)(h)\n",
    "h = Dense(DIM_HIDDEN_2, activation = 'relu')(h)\n",
    "h = Dropout(DROPOUT_RATE)(h)\n",
    "y = Dense(1)(h)\n",
    "\n",
    "model = Model(inputs = x, outputs = y)\n",
    "model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = LR)) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bidirectional_1/forward_lstm_1/kernel:0' shape=(300, 600) dtype=float32, numpy=\n",
       " array([[ 0.01636492, -0.02177608,  0.07274732, ...,  0.04867616,\n",
       "         -0.00235229,  0.03806137],\n",
       "        [-0.03361943,  0.06085576,  0.01252838, ..., -0.00084101,\n",
       "          0.03197478,  0.06178802],\n",
       "        [ 0.05570008,  0.04452485,  0.01821449, ...,  0.03646465,\n",
       "         -0.06384901, -0.06608415],\n",
       "        ...,\n",
       "        [ 0.04917453, -0.01154429, -0.06693958, ...,  0.07245913,\n",
       "         -0.02078933,  0.04338737],\n",
       "        [ 0.07734028, -0.0148826 ,  0.01956581, ...,  0.04428703,\n",
       "         -0.04972262,  0.03288473],\n",
       "        [ 0.05735902, -0.05284134, -0.08056441, ..., -0.03044537,\n",
       "          0.0580876 , -0.0694908 ]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/forward_lstm_1/recurrent_kernel:0' shape=(150, 600) dtype=float32, numpy=\n",
       " array([[-0.05960584, -0.04756306, -0.00860993, ..., -0.03443159,\n",
       "         -0.03795656,  0.01080187],\n",
       "        [ 0.02192086,  0.03145051, -0.02476786, ..., -0.05039116,\n",
       "          0.11127373, -0.02115226],\n",
       "        [-0.01647763, -0.04476822, -0.024984  , ..., -0.02825428,\n",
       "          0.02770377, -0.03906702],\n",
       "        ...,\n",
       "        [-0.0426243 ,  0.0222882 , -0.00634491, ..., -0.02677586,\n",
       "          0.05133695,  0.01981213],\n",
       "        [ 0.00478748,  0.05184765,  0.01237653, ..., -0.00782518,\n",
       "          0.02781466, -0.06423424],\n",
       "        [ 0.01425519,  0.04230332, -0.04208139, ..., -0.05568524,\n",
       "         -0.01730425,  0.02430265]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/forward_lstm_1/bias:0' shape=(600,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/backward_lstm_1/kernel:0' shape=(300, 600) dtype=float32, numpy=\n",
       " array([[-0.00725786,  0.05448329, -0.02407496, ..., -0.00148393,\n",
       "         -0.01111832, -0.00113975],\n",
       "        [ 0.00299619,  0.06364496,  0.07458274, ..., -0.03614898,\n",
       "          0.06204492,  0.00531545],\n",
       "        [ 0.07314847,  0.02539379, -0.04996998, ..., -0.07745741,\n",
       "         -0.03570243,  0.08091779],\n",
       "        ...,\n",
       "        [ 0.00837549,  0.05301332,  0.02627268, ..., -0.0595544 ,\n",
       "          0.00490462,  0.00082755],\n",
       "        [-0.06732073, -0.02933035, -0.07695731, ..., -0.01249075,\n",
       "         -0.06716101,  0.01919095],\n",
       "        [ 0.06028511,  0.06554671, -0.06547077, ..., -0.06991328,\n",
       "          0.03212936, -0.08109993]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/backward_lstm_1/recurrent_kernel:0' shape=(150, 600) dtype=float32, numpy=\n",
       " array([[-0.00018406,  0.02194584,  0.00805161, ...,  0.07511482,\n",
       "         -0.02236644, -0.03258092],\n",
       "        [-0.00510442, -0.04757094, -0.010186  , ...,  0.00287495,\n",
       "         -0.1267595 , -0.03675668],\n",
       "        [-0.00406448, -0.05497885,  0.02819502, ..., -0.03455515,\n",
       "          0.05876203,  0.0068348 ],\n",
       "        ...,\n",
       "        [ 0.00887736,  0.01967831, -0.02323291, ..., -0.04756251,\n",
       "          0.01194068,  0.06109508],\n",
       "        [-0.06274334, -0.06069148,  0.01381053, ...,  0.02694008,\n",
       "          0.04220983,  0.00917205],\n",
       "        [ 0.00764033,  0.02076494,  0.02958772, ...,  0.01926561,\n",
       "          0.06049636,  0.00027963]], dtype=float32)>,\n",
       " <tf.Variable 'bidirectional_1/backward_lstm_1/bias:0' shape=(600,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove-bilstm-150-150-0.1_le_220_em_300_lr_0.001_ep_10_ba_16 \n",
      "\n",
      "Train on 8320 samples, validate on 2080 samples\n",
      "Epoch 1/10\n",
      "8320/8320 [==============================] - 130s 16ms/sample - loss: 0.8540 - val_loss: 0.5761\n",
      "Epoch 2/10\n",
      "8320/8320 [==============================] - 127s 15ms/sample - loss: 0.6147 - val_loss: 0.5814\n",
      "Epoch 3/10\n",
      "8320/8320 [==============================] - 127s 15ms/sample - loss: 0.5405 - val_loss: 0.4755\n",
      "Epoch 4/10\n",
      "8320/8320 [==============================] - 129s 16ms/sample - loss: 0.5256 - val_loss: 0.5015\n",
      "Epoch 5/10\n",
      "8320/8320 [==============================] - 128s 15ms/sample - loss: 0.4827 - val_loss: 0.4556\n",
      "Epoch 6/10\n",
      "8320/8320 [==============================] - 128s 15ms/sample - loss: 0.4497 - val_loss: 0.4356\n",
      "Epoch 7/10\n",
      "8320/8320 [==============================] - 127s 15ms/sample - loss: 0.4205 - val_loss: 0.4647\n",
      "Epoch 8/10\n",
      "8320/8320 [==============================] - 127s 15ms/sample - loss: 0.3937 - val_loss: 0.4371\n",
      "Epoch 9/10\n",
      "8320/8320 [==============================] - 128s 15ms/sample - loss: 0.3551 - val_loss: 0.4572\n",
      "Epoch 10/10\n",
      "8320/8320 [==============================] - 128s 15ms/sample - loss: 0.3227 - val_loss: 0.4300\n"
     ]
    }
   ],
   "source": [
    "print(STAMP, '\\n')\n",
    "\n",
    "log = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs = NUM_EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    validation_split = 0.20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2501"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_train, batch_size = BATCH_SIZE)\n",
    "\n",
    "np.round_(mean_squared_error(y_train, y_pred), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4651"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, batch_size = BATCH_SIZE)\n",
    "\n",
    "MSE = np.round_(mean_squared_error(y_test, y_pred), 4)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(log.history)\n",
    "\n",
    "log_df.to_csv(f'./output/log_train_{STAMP}-mse-{MSE}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(f'./input/tokenizer_{STAMP}-mse-{MSE}.json', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii = False))\n",
    "\n",
    "model.save(f'./input/model_{STAMP}-mse-{MSE}.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./input/df_pre_train.csv')\n",
    "df_test = pd.read_csv('./input/df_pre_test.csv')\n",
    "\n",
    "input_train = process_df(df_train)\n",
    "input_test = process_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = tokenizer.texts_to_sequences(input_train)\n",
    "input_train = pad_sequences(input_train, maxlen = SEQ_LEN)\n",
    "\n",
    "input_test = tokenizer.texts_to_sequences(input_test)\n",
    "input_test = pad_sequences(input_test, maxlen = SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_feature'] = model.predict(input_train, batch_size = BATCH_SIZE)\n",
    "df_test['text_feature'] = model.predict(input_test, batch_size = BATCH_SIZE)\n",
    "\n",
    "df_train = df_train.drop(TEXT_COLS, axis = 1)\n",
    "df_test = df_test.drop(TEXT_COLS, axis = 1)\n",
    "\n",
    "df_train.to_csv(f'./input/df_final_train_{EMBED_TYPE}-{MODEL_TYPE}-mse-{MSE}.csv', index = False)\n",
    "df_test.to_csv(f'./input/df_final_test_{EMBED_TYPE}-{MODEL_TYPE}-mse-{MSE}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
